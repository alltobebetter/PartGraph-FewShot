# PartGraph: 基于部件图匹配的少样本学习

> 一种模拟人类认知的少样本图像分类方法

---

## 1. 研究动机

### 1.1 核心问题：为什么机器学习需要海量数据？

人类看3张猫的图片就能识别新的猫，但深度学习模型需要成千上万张。这个差距的根本原因是什么？

**传统深度学习的做法：**
```
输入图片 → CNN/ViT → 整体特征向量 [0.2, 0.5, 0.1, ...] → 分类器 → 猫/狗
```

这个特征向量是一个"黑盒"：
- 你不知道哪个维度代表什么
- 模型学到的是"统计相关性"而非"概念理解"
- 需要大量样本覆盖各种变化（角度、光照、姿态、遮挡...）

### 1.2 人类是怎么学习的？

认知科学研究表明，人类识别物体时会：

1. **分解**：把物体分解成有意义的部件（耳朵、眼睛、鼻子...）
2. **抽象**：每个部件形成独立的概念表征
3. **关联**：理解部件之间的空间和语义关系
4. **组合**：新类别 = 已知部件的新组合方式

```
人看3张猫图片时脑子里发生的事情：

图片1 → [三角耳朵] + [圆脸] + [竖瞳] + [胡须] + [毛茸茸身体]
图片2 → [三角耳朵] + [圆脸] + [竖瞳] + [胡须] + [毛茸茸身体]
图片3 → [三角耳朵] + [圆脸] + [竖瞳] + [胡须] + [毛茸茸身体]

归纳出规则：猫 = 这些部件按特定空间关系组合
```

**关键洞察：部件是可以跨类别复用的！**

- "圆眼睛"在猫、狗、兔子身上都有
- "毛茸茸"在很多动物身上都有
- 学习新类别时，只需要学习"新部件"和"新的组合方式"

这就是为什么人类能few-shot学习——我们在复用已有的部件知识。

---

## 2. 核心思想

### 2.1 方法概述

我们提出 **PartGraph**，一个模拟人类认知过程的少样本学习框架：

```
┌─────────────────────────────────────────────────────────────────┐
│                        PartGraph 框架                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   输入图像                                                        │
│      ↓                                                           │
│   ┌─────────────────────────────┐                               │
│   │  Part Discovery Module      │  ← 自动发现K个部件              │
│   │  (基于Slot Attention改进)    │     无需人工标注                │
│   └─────────────────────────────┘                               │
│      ↓                                                           │
│   K个部件表征 + 每个部件的空间位置                                  │
│      ↓                                                           │
│   ┌─────────────────────────────┐                               │
│   │  Relation Graph Module      │  ← 建模部件间的空间/语义关系     │
│   │  (图神经网络)                │                                │
│   └─────────────────────────────┘                               │
│      ↓                                                           │
│   部件关系图 (节点=部件, 边=关系)                                   │
│      ↓                                                           │
│   ┌─────────────────────────────┐                               │
│   │  Graph Matching Classifier  │  ← 基于图结构相似度分类          │
│   │  (图匹配网络)                │     而非简单向量距离             │
│   └─────────────────────────────┘                               │
│      ↓                                                           │
│   分类结果                                                        │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### 2.2 与现有方法的本质区别

| 维度 | 传统方法 (ProtoNet等) | 我们的方法 (PartGraph) |
|------|----------------------|----------------------|
| 表征形式 | 整体特征向量 | 结构化部件图 |
| 学什么 | 像素统计模式 | 部件 + 关系 |
| 分类依据 | 向量距离 | 图结构匹配 |
| 知识复用 | 无法复用 | 部件跨类别复用 |
| 可解释性 | 黑盒 | 可视化部件attention |

---

## 3. 技术方案详解

### 3.1 模块一：部件发现 (Part Discovery)

#### 3.1.1 为什么选择 Slot Attention？

Slot Attention (Locatello et al., NeurIPS 2020) 是一种无监督的物体/部件发现机制：

- **无需标注**：自动学习把图像分解成多个"槽位"(slots)
- **竞争机制**：不同槽位会竞争关注图像的不同区域
- **可微分**：可以端到端训练

#### 3.1.2 Slot Attention 原理

```
输入：特征图 F ∈ R^(H×W×D)  (比如来自ResNet的14×14×512)
输出：K个槽位表征 S ∈ R^(K×D)

算法：
1. 初始化K个槽位向量 s_1, s_2, ..., s_K (随机或可学习)

2. 重复 T 次迭代：
   a. 计算attention权重：
      A[i,j] = softmax_over_slots( dot(s_i, F_j) / sqrt(D) )
      
      注意：softmax是在slots维度上做的！
      这意味着每个位置j会被"分配"给某个slot
      
   b. 更新槽位：
      s_i = GRU(s_i, weighted_sum(F, A[i,:]))

3. 返回最终的槽位表征 S
```

**直观理解**：
- 每个slot像一个"探测器"，在图像上寻找自己负责的区域
- 通过竞争机制，不同slot会自动分工，关注不同部件
- 迭代过程让分配逐渐稳定

#### 3.1.3 我们的改进：Part-Aware Slot Attention

原版Slot Attention的问题：
1. 设计用于物体发现，不是部件发现
2. 没有利用部件的先验知识（比如部件通常是局部的、有一定大小的）

我们的改进：

```
改进1：局部性约束
- 在attention计算中加入位置编码
- 鼓励每个slot关注空间上连续的区域

改进2：尺度感知
- 使用多尺度特征图
- 不同slot可以关注不同尺度的部件（大部件如身体 vs 小部件如眼睛）

改进3：部件数量自适应
- 不是固定K个slot，而是学习哪些slot是"有效"的
- 通过一个置信度分数过滤掉空slot
```

### 3.2 模块二：关系建模 (Relation Graph)

#### 3.2.1 为什么需要关系建模？

光有部件还不够。考虑：
- 猫和老虎都有"条纹"，但组合方式不同
- 人脸的五官位置关系是固定的，打乱就不是人脸了

部件之间的**关系**是区分类别的关键信息。

#### 3.2.2 关系图的构建

```
节点：每个有效的部件slot
边：部件对之间的关系

边特征包括：
1. 空间关系：
   - 相对位置 (dx, dy)
   - 距离
   - 角度
   
2. 语义关系（可学习）：
   - 两个部件表征的交互特征
   - 通过MLP学习
```

#### 3.2.3 图神经网络更新

使用 Graph Attention Network (GAT) 更新节点表征：

```
对于每个节点 i：
1. 聚合邻居信息：
   h_i' = Σ_j α_ij * W * h_j
   
   其中 α_ij 是attention权重，考虑边特征
   
2. 更新节点：
   h_i = h_i + MLP(h_i')
```

这样每个部件的表征会融入其"上下文"——即它和其他部件的关系。

### 3.3 模块三：图匹配分类 (Graph Matching)

#### 3.3.1 传统 vs 我们的分类方式

**传统 Prototypical Network：**
```
支持集 S = {(x_1, y_1), ..., (x_n, y_n)}
对每个类别 c，计算原型：p_c = mean(f(x_i) for x_i in class c)
查询样本 q 的预测：argmax_c similarity(f(q), p_c)
```

问题：平均操作丢失了结构信息

**我们的方法：**
```
支持集中每个样本 → 部件图 G_i
查询样本 → 部件图 G_q

分类 = 找到和 G_q 最匹配的类别的图结构
```

#### 3.3.2 图匹配的实现

我们使用可微分的图匹配网络：

```
1. 节点匹配：
   - 计算查询图和支持图之间的节点相似度矩阵
   - M[i,j] = similarity(q_node_i, s_node_j)
   
2. 结构匹配：
   - 不只看节点，还要看边的匹配程度
   - 使用 Graph Matching Network 或 Optimal Transport
   
3. 最终得分：
   - 综合节点匹配和结构匹配
   - 选择得分最高的类别
```

---

## 4. 训练策略

### 4.1 Episode-based 训练

遵循标准的 few-shot learning 训练范式：

```
每个 episode：
1. 从训练集采样 N 个类别（N-way）
2. 每个类别采样 K 个支持样本 + Q 个查询样本（K-shot）
3. 用支持集构建类别的部件图原型
4. 预测查询样本的类别
5. 计算损失，反向传播
```

### 4.2 损失函数

```
总损失 = L_classification + λ1 * L_reconstruction + λ2 * L_diversity

L_classification: 标准交叉熵损失
L_reconstruction: 从部件重建原图（确保部件有意义）
L_diversity: 鼓励不同slot关注不同区域（避免collapse）
```

### 4.3 预训练策略

```
阶段1：在大数据集上预训练backbone（如ImageNet）
阶段2：在base classes上训练部件发现模块
阶段3：端到端fine-tune整个框架
```

---

## 5. 实验计划

### 5.1 数据集

| 数据集 | 用途 | 特点 |
|--------|------|------|
| Omniglot | 初步验证 | 字符识别，天然有部件结构 |
| miniImageNet | 主实验 | 标准few-shot benchmark |
| tieredImageNet | 泛化测试 | 更大规模，类别层次结构 |
| CUB-200 | 细粒度测试 | 鸟类，部件差异细微 |

### 5.2 对比方法

- ProtoNet (基线)
- MAML
- RelationNet
- FEAT
- DeepEMD
- CORL (最相关的compositional方法)

### 5.3 评估指标

- N-way K-shot 准确率（标准指标）
- 部件发现质量（可视化 + IoU如果有标注）
- 组合泛化能力（专门设计的测试）

### 5.4 消融实验

1. 有无部件发现模块
2. 有无关系图模块
3. 不同slot数量K的影响
4. 不同图匹配方法的比较
5. 预训练策略的影响

---

## 6. 预期贡献

1. **新框架**：首个将无监督部件发现 + 关系图 + 图匹配统一用于few-shot learning的方法

2. **新视角**：从"学习更好的特征"转向"学习更好的结构化表征"

3. **可解释性**：部件attention map提供了模型决策的可视化解释

4. **理论洞察**：解释为什么部件级表征能提升few-shot能力（部件复用假说）

---

## 7. 时间规划

| 阶段 | 时间 | 目标 |
|------|------|------|
| 阶段1 | 第1-2周 | 搭建基础框架，复现baseline |
| 阶段2 | 第3-6周 | 实现Part Discovery模块，在Omniglot验证 |
| 阶段3 | 第7-10周 | 实现Relation Graph模块 |
| 阶段4 | 第11-14周 | 实现Graph Matching，完整pipeline |
| 阶段5 | 第15-18周 | miniImageNet实验，调参优化 |
| 阶段6 | 第19-22周 | 消融实验，可视化分析 |
| 阶段7 | 第23-26周 | 论文撰写 |

---

## 8. 风险与应对

| 风险 | 可能性 | 应对策略 |
|------|--------|----------|
| Slot Attention无法发现有意义的部件 | 中 | 加入更强的先验约束；使用预训练的部件检测器作为初始化 |
| 图匹配计算量太大 | 中 | 使用近似算法；限制图的大小 |
| 效果不如预期 | 中 | 准备多个备选方案；可以退化为更简单的版本发表 |
| 训练不稳定 | 低 | 分阶段训练；使用预训练初始化 |

---

## 9. 参考文献

1. Locatello et al. "Object-Centric Learning with Slot Attention" NeurIPS 2020
2. Snell et al. "Prototypical Networks for Few-shot Learning" NeurIPS 2017
3. Tokmakov et al. "Learning Compositional Representations for Few-Shot Recognition" ICCV 2019
4. He et al. "CORL: Compositional Representation Learning for Few-Shot Classification" WACV 2023
5. Veličković et al. "Graph Attention Networks" ICLR 2018
