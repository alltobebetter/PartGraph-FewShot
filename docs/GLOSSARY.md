# 专业术语详解

本文档详细解释 PartGraph 项目中涉及的所有专业术语，帮助读者全面理解项目的技术背景。

---

## 目录

1. [Few-Shot Learning 相关术语](#1-few-shot-learning-相关术语)
2. [深度学习基础术语](#2-深度学习基础术语)
3. [注意力机制相关术语](#3-注意力机制相关术语)
4. [图神经网络相关术语](#4-图神经网络相关术语)
5. [训练与优化相关术语](#5-训练与优化相关术语)
6. [评估指标相关术语](#6-评估指标相关术语)
7. [数据集相关术语](#7-数据集相关术语)

---

## 1. Few-Shot Learning 相关术语

### 1.1 Few-Shot Learning（少样本学习）

**定义**: 一种机器学习范式，目标是让模型能够从极少量的标注样本（通常 1-5 个）中学习识别新类别。

**背景**: 传统深度学习需要大量标注数据（数千到数百万），而人类只需看几个例子就能学会新概念。Few-Shot Learning 试图弥合这一差距。

**分类**:
- **1-shot Learning**: 每个新类别只有 1 个样本
- **5-shot Learning**: 每个新类别有 5 个样本
- **Zero-shot Learning**: 没有样本，只有类别的语义描述

**应用场景**:
- 医学影像诊断（罕见疾病样本稀少）
- 药物发现（新化合物数据有限）
- 个性化推荐（新用户数据不足）

### 1.2 N-way K-shot

**定义**: Few-Shot Learning 的标准评估协议。

**含义**:
- **N-way**: 每个测试任务（episode）包含 N 个类别
- **K-shot**: 每个类别提供 K 个标注样本作为参考

**示例**: 5-way 5-shot 表示：
- 随机选择 5 个类别
- 每个类别给 5 张图片作为"参考答案"
- 然后对新图片进行分类

**常见设置**:
- 5-way 1-shot（最难）
- 5-way 5-shot（标准）
- 5-way 10-shot / 20-shot（较简单）

### 1.3 Episode（情节/任务）

**定义**: Few-Shot Learning 中的一个独立测试任务。

**组成**:

- **Support Set（支持集）**: 提供的参考样本，用于"学习"新类别
- **Query Set（查询集）**: 需要分类的测试样本

**示例**: 一个 5-way 5-shot episode
```
Support Set: 5 类 × 5 张 = 25 张参考图片
Query Set: 5 类 × 15 张 = 75 张待分类图片
```

**评估方式**: 运行数百个 episode（通常 600-1000 个），计算平均准确率。

### 1.4 Base Classes / Novel Classes

**定义**: Few-Shot Learning 中数据集的标准划分方式。

**Base Classes（基础类别）**:

- 用于训练模型
- 模型可以看到这些类别的大量样本
- 目的是学习通用的特征表示

**Novel Classes（新类别）**:

- 用于测试模型
- 训练时**完全没有见过**这些类别
- 测试时只提供少量样本（K-shot）

**为什么这样划分**:
- 测试模型是否真正学会了"学习如何学习"
- 而不是简单地记住训练类别

**CUB-200 的划分**:
- Base Classes: 类别 1-100（约 5,864 张图片）
- Novel Classes: 类别 101-200（约 5,924 张图片）

### 1.5 Prototypical Network（原型网络）

**定义**: 一种经典的 Few-Shot Learning 方法，由 Snell et al. 在 NeurIPS 2017 提出。

**核心思想**: 每个类别可以用一个"原型"（prototype）来表示，原型就是该类别所有样本特征的平均值。

**工作流程**:

```
1. 对 Support Set 中每个类别的样本提取特征
2. 计算每个类别的原型（特征平均值）
3. 对 Query 样本提取特征
4. 计算 Query 特征到各原型的距离
5. 分配给距离最近的类别
```

**数学表示**:
```
原型: c_k = (1/|S_k|) × Σ f(x_i)，其中 x_i ∈ S_k
分类: y = argmin_k d(f(x_query), c_k)
```

**优点**:
- 简单有效
- 不需要在测试时更新模型参数
- 计算效率高

### 1.6 Metric Learning（度量学习）

**定义**: 一类机器学习方法，目标是学习一个好的距离度量（或相似度度量），使得同类样本距离近，异类样本距离远。

**与 Few-Shot 的关系**: Prototypical Network 就是一种度量学习方法，它学习一个特征空间，在这个空间中可以通过简单的距离计算进行分类。

**常用距离度量**:
- **欧氏距离**: d(x,y) = ||x - y||_2
- **余弦距离**: d(x,y) = 1 - cos(x,y)
- **马氏距离**: 考虑特征相关性的距离

### 1.7 Meta-Learning（元学习）

**定义**: "学习如何学习"的机器学习范式。

**核心思想**: 不是学习解决某个具体任务，而是学习一种能够快速适应新任务的能力。

**与 Few-Shot 的关系**: Few-Shot Learning 可以看作 Meta-Learning 的一个应用场景。

**代表方法**:
- **MAML**: 学习一个好的模型初始化，使得在新任务上只需少量梯度更新
- **Prototypical Network**: 学习一个好的特征空间
- **Matching Network**: 学习一个端到端的匹配函数

---

## 2. 深度学习基础术语

### 2.1 Backbone（骨干网络）

**定义**: 深度学习模型中负责提取特征的主干网络。

**作用**: 将原始输入（如图像）转换为高维特征表示。

**常见 Backbone**:

- **ResNet**: 残差网络，通过跳跃连接解决深层网络训练困难
- **VGG**: 使用小卷积核的深层网络
- **ViT**: Vision Transformer，将 Transformer 应用于图像

**本项目使用**: ResNet-18

- 18 层深度
- ImageNet 预训练
- 输出 14×14×256 的特征图（使用 layer3）

### 2.2 ResNet（残差网络）

**定义**: 由 He et al. 在 2015 年提出的深度卷积神经网络架构。

**核心创新**: 残差连接（Residual Connection）

```
输出 = F(x) + x

其中 F(x) 是网络学习的残差，x 是输入的恒等映射
```

**解决的问题**: 深层网络的退化问题（更深的网络反而效果更差）

**变体**:
- ResNet-18: 18 层，参数量约 11M
- ResNet-34: 34 层
- ResNet-50: 50 层，使用 Bottleneck 结构
- ResNet-101/152: 更深的版本

### 2.3 Feature Map（特征图）

**定义**: 卷积神经网络中间层的输出，是一个三维张量。

**维度**: (C, H, W)

- C: 通道数（特征维度）
- H: 高度
- W: 宽度

**示例**: ResNet-18 的 layer3 输出
- 输入: 224×224×3 的 RGB 图像
- 输出: 14×14×256 的特征图
- 每个 14×14 的位置对应原图的一个区域（感受野）

### 2.4 Embedding（嵌入）

**定义**: 将高维或离散的数据映射到低维连续向量空间的表示。

**作用**: 
- 降维
- 捕捉语义相似性（相似的输入有相似的嵌入）

**本项目中的嵌入**:
- Slot 嵌入: 每个 slot 是一个 256 维向量，表示一个部件
- 位置嵌入: 编码空间位置信息

### 2.5 Encoder / Decoder（编码器/解码器）

**Encoder（编码器）**:
- 将输入压缩为紧凑的表示
- 本项目: ResNet + Slot Attention 将图像编码为 8 个 slot

**Decoder（解码器）**:
- 将紧凑表示还原为原始形式
- 本项目: 从 slot 重建原始图像（用于辅助训练）

### 2.6 Latent Space（潜在空间）

**定义**: 模型学习到的中间表示空间。

**特点**:
- 比原始输入空间更紧凑
- 捕捉数据的本质特征
- 相似的输入在潜在空间中距离近

**本项目**: Slot 特征构成的空间就是一种潜在空间。

---

## 3. 注意力机制相关术语

### 3.1 Attention Mechanism（注意力机制）

**定义**: 一种让模型"关注"输入中重要部分的机制。

**核心思想**: 不是平等对待所有输入，而是根据重要性分配不同的权重。

**基本形式**:
```
Attention(Q, K, V) = softmax(QK^T / √d) × V

Q: Query（查询）
K: Key（键）
V: Value（值）
d: 维度（用于缩放）
```

**直观理解**: 
- Query 是"我在找什么"
- Key 是"这里有什么"
- Value 是"具体内容"
- 通过 Q 和 K 的匹配程度决定从 V 中取多少信息

### 3.2 Self-Attention（自注意力）

**定义**: Query、Key、Value 都来自同一个输入的注意力机制。

**作用**: 让输入的每个位置都能关注到其他所有位置，捕捉全局依赖关系。

**应用**: Transformer 的核心组件。

### 3.3 Slot Attention（槽注意力）

**定义**: 由 Locatello et al. (NeurIPS 2020) 提出的一种注意力机制，用于无监督的物体/部件发现。

**核心思想**: 
- 维护 K 个"槽"（slot），每个槽学习关注输入的不同部分
- 通过竞争机制，不同槽自动分工

**与普通 Attention 的区别**:
```
普通 Attention: softmax 在 Key 维度
Slot Attention: softmax 在 Slot 维度（竞争机制）
```

**工作流程**:
```
1. 初始化 K 个 slot 向量
2. 迭代 T 次:
   a. 计算每个位置对每个 slot 的注意力权重
   b. softmax 在 slot 维度（每个位置被"分配"给某个 slot）
   c. 根据注意力权重更新 slot
3. 输出 K 个 slot，每个代表一个物体/部件
```

**竞争机制的意义**:
- 每个像素位置会被分配给最匹配的 slot
- 不同 slot 被迫关注不同区域
- 实现无监督的分割/分解

### 3.4 Multi-Head Attention（多头注意力）

**定义**: 将注意力机制并行执行多次，每次使用不同的投影矩阵。

**目的**: 让模型能够同时关注不同类型的信息。

**公式**:

```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) × W_O
head_i = Attention(Q×W_Q^i, K×W_K^i, V×W_V^i)
```

**本项目**: GNN 层使用 4 头注意力。

### 3.5 Attention Map（注意力图）

**定义**: 注意力权重的可视化，显示模型关注输入的哪些部分。

**本项目**: 每个 slot 的 attention map 显示该 slot 关注图像的哪个区域，可用于可视化部件发现结果。

---

## 4. 图神经网络相关术语

### 4.1 Graph Neural Network, GNN（图神经网络）

**定义**: 一类能够处理图结构数据的神经网络。

**图的组成**:
- **节点（Node/Vertex）**: 图中的实体
- **边（Edge）**: 节点之间的关系
- **节点特征**: 每个节点的属性向量
- **边特征**: 每条边的属性向量

**核心操作**: 消息传递（Message Passing）
```
对于每个节点:
1. 收集邻居节点的信息（消息）
2. 聚合这些消息
3. 更新自己的表示
```

**本项目应用**: 
- 节点 = Slot（部件）
- 边 = 部件之间的空间关系
- 通过 GNN 让部件之间交换信息

### 4.2 Message Passing（消息传递）

**定义**: GNN 的核心机制，节点通过边与邻居交换信息。

**一般形式**:

```
m_ij = MESSAGE(h_i, h_j, e_ij)  # 从 j 到 i 的消息
m_i = AGGREGATE({m_ij : j ∈ N(i)})  # 聚合所有邻居的消息
h_i' = UPDATE(h_i, m_i)  # 更新节点表示
```

**本项目**: 每个 slot 收集其他 slot 的信息，了解"其他部件在关注什么"。

### 4.3 Graph Attention Network, GAT（图注意力网络）

**定义**: 使用注意力机制的图神经网络，由 Veličković et al. (ICLR 2018) 提出。

**核心思想**: 不同邻居的重要性不同，用注意力权重区分。

**公式**:
```
α_ij = softmax(LeakyReLU(a^T [W×h_i || W×h_j]))
h_i' = σ(Σ α_ij × W × h_j)
```

**本项目**: GNN 层使用 GAT 风格的注意力，并加入边特征作为偏置。

### 4.4 Edge Feature（边特征）

**定义**: 描述图中边的属性的向量。

**本项目的边特征**:
- 相对位置 (dx, dy)
- 欧氏距离
- 方向角
- 语义相似度（slot 特征的余弦相似度）
- 是否邻近（距离阈值判断）

### 4.5 Graph Matching（图匹配）

**定义**: 比较两个图的结构相似度的方法。

**应用于 Few-Shot**:
- Support 样本 → 部件图
- Query 样本 → 部件图
- 通过图匹配计算相似度进行分类

**优势**: 能够利用部件间的结构关系，而不仅仅是特征相似度。

**本项目状态**: 计划实现，是下一步的重点方向。

### 4.6 GNN-in-the-Loop

**定义**: 本项目提出的机制，在 Slot Attention 的每次迭代中嵌入 GNN 消息传递。

**与传统方法的区别**:

```
传统: Slot Attention → GNN（串行）
我们: 每次 Slot Attention 迭代后都进行 GNN 消息传递（嵌入式）
```

**目的**: 让 slot 在发现部件的过程中就能感知其他 slot 的状态，实现协作式部件发现。

---

## 5. 训练与优化相关术语

### 5.1 Loss Function（损失函数）

**定义**: 衡量模型预测与真实值之间差距的函数，训练的目标是最小化损失。

**本项目使用的损失函数**:

1. **分类损失（Classification Loss）**
   - 交叉熵损失
   - 衡量分类预测的准确性
   - 权重: 1.0

2. **重建损失（Reconstruction Loss）**
   - MSE（均方误差）
   - 衡量从 slot 重建图像的质量
   - 权重: 0.1
   - 作用: 确保 slot 捕捉到有意义的视觉信息

3. **多样性损失（Diversity Loss）**
   - 衡量不同 slot 的 attention 重叠程度
   - 权重: 0.01
   - 作用: 鼓励不同 slot 关注不同区域，避免 slot collapse

### 5.2 Cross-Entropy Loss（交叉熵损失）

**定义**: 分类任务中最常用的损失函数。

**公式**:
```
L = -Σ y_i × log(p_i)

y_i: 真实标签（one-hot）
p_i: 预测概率
```

**直观理解**: 惩罚模型对正确类别预测的概率不够高。

### 5.3 MSE Loss（均方误差损失）

**定义**: 回归任务中常用的损失函数。

**公式**:
```
L = (1/n) × Σ (y_i - ŷ_i)²
```

**本项目应用**: 衡量重建图像与原图的像素级差异。

### 5.4 Optimizer（优化器）

**定义**: 根据损失函数的梯度更新模型参数的算法。

**本项目使用**: AdamW
- Adam 的改进版本
- 加入了权重衰减（weight decay）正则化
- 学习率: 1e-4（Baseline）/ 5e-5（GNN）

### 5.5 Learning Rate（学习率）

**定义**: 控制每次参数更新步长的超参数。

**影响**:

- 太大: 训练不稳定，可能发散
- 太小: 收敛太慢
- 合适: 稳定收敛到好的解

**本项目**: 
- Baseline: 1e-4
- GNN: 5e-5（更小，因为模型更复杂）

### 5.6 Learning Rate Scheduler（学习率调度器）

**定义**: 在训练过程中动态调整学习率的策略。

**本项目使用**: Cosine Annealing
- 学习率按余弦曲线从初始值逐渐降到接近 0
- 有助于在训练后期精细调整

### 5.7 Batch Size（批大小）

**定义**: 每次参数更新使用的样本数量。

**影响**:
- 大 batch: 梯度估计更准确，但需要更多显存
- 小 batch: 显存友好，但梯度噪声大

**本项目**: 16（受 GPU 显存限制）

### 5.8 Epoch（轮次）

**定义**: 遍历整个训练集一次称为一个 epoch。

**本项目**: 最多 50 个 epoch，但有早停机制。

### 5.9 Early Stopping（早停）

**定义**: 当模型性能不再提升时提前停止训练，防止过拟合。

**本项目实现**: 连续 3 次训练准确率 ≥99% 时自动停止。

### 5.10 Overfitting（过拟合）

**定义**: 模型在训练集上表现很好，但在测试集上表现差。

**原因**: 模型"记住"了训练数据的噪声，而非学习通用规律。

**本项目的应对**:
- 多任务学习（重建损失作为正则化）
- 早停
- 权重衰减

### 5.11 Gradient Clipping（梯度裁剪）

**定义**: 限制梯度的最大范数，防止梯度爆炸。

**本项目**: 梯度范数裁剪到 1.0。

### 5.12 Pretrained Model（预训练模型）

**定义**: 在大规模数据集上预先训练好的模型。

**本项目**: 使用 ImageNet 预训练的 ResNet-18 作为 backbone。

**优势**: 
- 已经学习了通用的视觉特征
- 加速收敛
- 提升性能

---

## 6. 评估指标相关术语

### 6.1 Accuracy（准确率）

**定义**: 正确预测的样本数占总样本数的比例。

**公式**:
```
Accuracy = 正确预测数 / 总样本数 × 100%
```

**本项目**: 5-way 5-shot 准确率，在 600 个 episode 上的平均值。

### 6.2 Confidence Interval（置信区间）

**定义**: 对真实值的估计范围，表示估计的不确定性。

**本项目**: 报告 95% 置信区间
```
CI = 1.96 × 标准差 / √(episode数)
```

**示例**: 79.24% ± 0.79% 表示真实准确率有 95% 的概率在 78.45% 到 80.03% 之间。

### 6.3 Ablation Study（消融实验）

**定义**: 通过移除或替换模型的某个组件，验证该组件的贡献。

**本项目的消融实验**:
- 有/无 Slot Attention
- 有/无 GNN-in-the-Loop
- 不同 slot 数量

### 6.4 Baseline（基线）

**定义**: 用于比较的参考方法，通常是简单或已有的方法。

**本项目的 Baseline**:
- ResNet Backbone + Prototypical Network（无 Slot Attention）
- 准确率: 47.43%

### 6.5 SOTA (State-of-the-Art)

**定义**: 当前最好的方法/结果。

**CUB-200 5-way 5-shot SOTA**: 约 85%+

---

## 7. 数据集相关术语

### 7.1 CUB-200-2011

**全称**: Caltech-UCSD Birds 200-2011

**内容**: 
- 200 种北美鸟类
- 约 12,000 张图像
- 每张图像有边界框和部件标注（本项目未使用部件标注）

**特点**: 细粒度分类数据集，类别间差异小，适合测试部件级表征。

### 7.2 miniImageNet

**定义**: ImageNet 的子集，专门用于 Few-Shot Learning 研究。

**内容**:
- 100 个类别
- 每类 600 张图像
- 标准划分: 64 训练 / 16 验证 / 20 测试

**地位**: Few-Shot Learning 最常用的 benchmark。

### 7.3 tieredImageNet

**定义**: 另一个 ImageNet 子集，比 miniImageNet 更大。

**内容**:
- 608 个类别
- 按语义层次划分（训练和测试类别来自不同的高层类别）

**特点**: 测试更强的泛化能力。

### 7.4 Fine-Grained Classification（细粒度分类）

**定义**: 区分同一大类下的子类别，如区分不同品种的鸟、不同型号的汽车。

**挑战**: 类别间差异小，需要关注细节特征。

**与本项目的关系**: CUB-200 是细粒度数据集，部件级表征特别适合这类任务。

### 7.5 Data Augmentation（数据增强）

**定义**: 通过对训练数据进行变换来增加数据多样性。

**本项目使用的增强**:
- 随机裁剪
- 随机水平翻转
- 颜色抖动（亮度、对比度、饱和度）

**作用**: 提升模型的泛化能力，减少过拟合。

### 7.6 Normalization（归一化）

**定义**: 将数据缩放到特定范围或分布。

**本项目**: 使用 ImageNet 的均值和标准差进行归一化
```
mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]
```

---

## 附录：缩写对照表

| 缩写 | 全称 | 中文 |
|------|------|------|
| FSL | Few-Shot Learning | 少样本学习 |
| GNN | Graph Neural Network | 图神经网络 |
| GAT | Graph Attention Network | 图注意力网络 |
| CNN | Convolutional Neural Network | 卷积神经网络 |
| MLP | Multi-Layer Perceptron | 多层感知机 |
| ReLU | Rectified Linear Unit | 修正线性单元 |
| SGD | Stochastic Gradient Descent | 随机梯度下降 |
| MSE | Mean Squared Error | 均方误差 |
| SOTA | State-of-the-Art | 最先进的 |
| IoU | Intersection over Union | 交并比 |
| GPU | Graphics Processing Unit | 图形处理单元 |
| CUDA | Compute Unified Device Architecture | NVIDIA 并行计算平台 |

---

*文档版本: 1.0*  
*最后更新: 2025年11月30日*
