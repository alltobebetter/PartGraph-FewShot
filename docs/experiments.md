# 实验设计文档

## 1. 实验目标

验证以下假设：
1. **部件发现有效**：Slot Attention能自动发现有意义的部件
2. **关系建模有用**：加入部件关系能提升分类准确率
3. **图匹配优于向量距离**：结构化匹配比简单的原型距离更好
4. **组合泛化**：模型能利用已知部件识别新类别

---

## 2. 数据集

### 2.1 Omniglot（初步验证）

- **内容**：1623个手写字符类别，每类20个样本
- **特点**：
  - 字符天然有笔画结构（类似部件）
  - 数据量小，训练快
  - 适合快速验证想法
- **划分**：
  - 训练：964类
  - 验证：100类
  - 测试：559类

### 2.2 miniImageNet（主实验）

- **内容**：100个类别，每类600张图片
- **特点**：
  - 标准few-shot benchmark
  - 自然图像，更有挑战性
- **划分**：
  - 训练：64类
  - 验证：16类
  - 测试：20类

### 2.3 tieredImageNet（泛化测试）

- **内容**：608个类别，按WordNet层次组织
- **特点**：
  - 训练/测试类别来自不同的高层类别
  - 测试模型的泛化能力
- **划分**：
  - 训练：351类
  - 验证：97类
  - 测试：160类

### 2.4 CUB-200-2011（细粒度测试）

- **内容**：200种鸟类
- **特点**：
  - 细粒度分类，类间差异小
  - 有部件标注（可用于分析）
- **用途**：测试部件发现在细粒度场景的效果

---

## 3. 评估协议

### 3.1 标准 Few-shot 评估

```
设置：N-way K-shot
- N = 5 (5个类别)
- K = 1 或 5 (每类1或5个支持样本)
- 查询样本：每类15个

评估：
- 随机采样600个episode
- 报告平均准确率和95%置信区间
```

### 3.2 我们额外的评估

**组合泛化测试**：
```
设计思路：
1. 在训练集中，某些部件组合从未出现
2. 在测试时，要求模型识别这些新组合

例如（假设有部件标注）：
- 训练时见过：红色+圆形，蓝色+方形
- 测试时要求识别：红色+方形，蓝色+圆形
```

**部件发现质量评估**：
```
如果有部件标注（如CUB数据集）：
- 计算slot attention和真实部件的IoU
- 分析每个slot是否对应有意义的部件

如果没有标注：
- 人工检查可视化结果
- 统计slot的激活区域是否一致
```

---

## 4. 对比方法

### 4.1 经典方法

| 方法 | 年份 | 会议 | 特点 |
|------|------|------|------|
| ProtoNet | 2017 | NeurIPS | 原型网络，简单有效 |
| MAML | 2017 | ICML | 元学习，学习初始化 |
| RelationNet | 2018 | CVPR | 学习比较函数 |
| MatchingNet | 2016 | NeurIPS | attention-based |

### 4.2 近期SOTA

| 方法 | 年份 | 会议 | 特点 |
|------|------|------|------|
| FEAT | 2020 | CVPR | Transformer增强 |
| DeepEMD | 2020 | CVPR | 最优传输匹配 |
| Meta-Baseline | 2020 | ICLR | 强预训练baseline |
| P>M>F | 2022 | CVPR | 预训练+元学习+微调 |

### 4.3 最相关的方法

| 方法 | 年份 | 特点 | 和我们的区别 |
|------|------|------|-------------|
| CORL | 2023 | 组合表征 | 没有显式部件发现 |
| Compositional Proto | 2023 | 属性组合 | 需要人工属性标注 |
| Tokmakov 2019 | 正则化分解 | 没有关系建模 |

---

## 5. 消融实验

### 5.1 模块消融

| 实验 | Part Discovery | Relation Graph | Graph Matching | 预期 |
|------|----------------|----------------|----------------|------|
| Baseline | ❌ | ❌ | ❌ | 最低 |
| +Part | ✅ | ❌ | ❌ | 提升 |
| +Part+Relation | ✅ | ✅ | ❌ | 进一步提升 |
| Full (Ours) | ✅ | ✅ | ✅ | 最高 |

### 5.2 部件数量消融

| K (slot数量) | 预期效果 |
|--------------|----------|
| 2 | 太少，无法捕捉细节 |
| 4 | 可能不够 |
| 8 | 预期最佳 |
| 16 | 可能过多，有冗余 |
| 32 | 过多，可能collapse |

### 5.3 Slot Attention 迭代次数

| 迭代次数 T | 预期效果 |
|------------|----------|
| 1 | 分配不够精确 |
| 3 | 预期最佳 |
| 5 | 可能过拟合 |
| 7 | 计算量大，收益小 |

### 5.4 图匹配方法对比

| 方法 | 特点 |
|------|------|
| 简单平均 | 把所有slot平均成一个向量 |
| 最大池化 | 取最相似的slot |
| 软匹配 (Sinkhorn) | 可微分的最优匹配 |
| 硬匹配 (Hungarian) | 精确匹配，但不可微 |

---

## 6. 可视化分析

### 6.1 部件发现可视化

```
对于每张图片，展示：
1. 原图
2. 每个slot的attention热力图
3. 所有slot叠加的分割结果
```

### 6.2 图结构可视化

```
展示：
1. 部件图的节点（部件位置）
2. 边的连接关系
3. 不同类别的典型图结构
```

### 6.3 匹配过程可视化

```
展示：
1. 查询图和支持图
2. 节点匹配对应关系
3. 匹配分数
```

---

## 7. 预期结果

### 7.1 miniImageNet 5-way

| 方法 | 1-shot | 5-shot |
|------|--------|--------|
| ProtoNet | 49.4% | 68.2% |
| MAML | 48.7% | 63.1% |
| RelationNet | 50.4% | 65.3% |
| FEAT | 55.2% | 71.7% |
| DeepEMD | 65.9% | 82.4% |
| **Ours (预期)** | **58-62%** | **74-78%** |

### 7.2 为什么我们可能不如DeepEMD？

DeepEMD使用了：
- 更大的backbone (ResNet-12)
- 更复杂的预训练策略
- 图像级别的最优传输

我们的优势在于：
- 更好的可解释性
- 更强的组合泛化能力
- 更少的计算量

---

## 8. 实验时间估计

| 阶段 | 实验内容 | GPU时间 | 说明 |
|------|----------|---------|------|
| 1 | Omniglot验证 | 2-4小时 | T4足够 |
| 2 | miniImageNet主实验 | 1-2天 | 需要更好的GPU |
| 3 | 消融实验 | 3-5天 | 多组实验 |
| 4 | tieredImageNet | 2-3天 | 数据量大 |
| 5 | CUB细粒度 | 1天 | |
| 6 | 可视化分析 | 1天 | |

**总计**：约2周GPU时间（如果用T4可能需要更长）
